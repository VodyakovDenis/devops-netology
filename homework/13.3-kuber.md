# Домашнее задание к занятию «Как работает сеть в K8s»

### Цель задания

Настроить сетевую политику доступа к подам.

### Чеклист готовности к домашнему заданию

1. Кластер K8s с установленным сетевым плагином Calico.

```bash
root@test-k8s1:~/hw3.3# kubectl get nodes -owide
NAME        STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
test-k8s1   Ready    control-plane   2d22h   v1.28.4   10.10.3.91    <none>        Ubuntu 22.04.3 LTS   5.15.0-89-generic   containerd://1.7.2
test-k8s2   Ready    worker          19m     v1.28.4   10.10.3.92    <none>        Ubuntu 22.04.3 LTS   5.15.0-89-generic   containerd://1.7.2
test-k8s3   Ready    worker          19m     v1.28.4   10.10.3.93    <none>        Ubuntu 22.04.3 LTS   5.15.0-89-generic   containerd://1.7.2
test-k8s4   Ready    worker          19m     v1.28.4   10.10.3.94    <none>        Ubuntu 22.04.3 LTS   5.15.0-89-generic   containerd://1.7.2
test-k8s5   Ready    worker          18m     v1.28.4   10.10.3.95    <none>        Ubuntu 22.04.3 LTS   5.15.0-89-generic   containerd://1.7.2
root@test-k8s1:~/hw3.3# kubectl get networkpolicies.networking.k8s.io -A
NAMESPACE          NAME              POD-SELECTOR     AGE
calico-apiserver   allow-apiserver   apiserver=true   2d22h

```

PS> за основу был взят кластер развернутый из прошлой домашней работы с одной мастер нодой.

---


### Задание 1. Создать сетевую политику или несколько политик для обеспечения доступа

1. Создать deployment'ы приложений frontend, backend и cache и соответсвующие сервисы.
2. В качестве образа использовать network-multitool.
3. Разместить поды в namespace App.
4. Создать политики, чтобы обеспечить доступ frontend -> backend -> cache. Другие виды подключений должны быть запрещены.
5. Продемонстрировать, что трафик разрешён и запрещён.

---

### Решение

```bash
root@test-k8s1:~/hw3.3# kubectl create namespace app
namespace/app created
root@test-k8s1:~/hw3.3# kubectl apply -f frontend.yaml -f backend.yaml -f cache.yaml -f networkpolicy.yml
deployment.apps/frontend created
service/front-svc created
deployment.apps/backend created
service/back-svc created
deployment.apps/cache created
service/cache-svc created
networkpolicy.networking.k8s.io/app-deny-ingress created
networkpolicy.networking.k8s.io/backend created
networkpolicy.networking.k8s.io/cache created
```

```bash
root@test-k8s1:~/hw3.3# kubectl get pods -A -owide
NAMESPACE          NAME                                       READY   STATUS             RESTARTS         AGE     IP               NODE        NOMINATED NODE   READINESS GATES
app                backend-ffdd87cf6-bpfzt                    1/1     Running            0                42s     10.244.187.202   test-k8s3   <none>           <none>
app                backend-ffdd87cf6-vgwd9                    1/1     Running            0                42s     10.244.187.200   test-k8s3   <none>           <none>
app                cache-79b74c6c94-6vlws                     1/1     Running            0                42s     10.244.187.204   test-k8s3   <none>           <none>
app                cache-79b74c6c94-slnbs                     1/1     Running            0                42s     10.244.187.203   test-k8s3   <none>           <none>
app                frontend-856666b494-ntm42                  1/1     Running            0                42s     10.244.187.199   test-k8s3   <none>           <none>
app                frontend-856666b494-w87nk                  1/1     Running            0                42s     10.244.187.201   test-k8s3   <none>           <none>
calico-apiserver   calico-apiserver-85ffbc4979-25r8q          1/1     Running            2 (31m ago)      2d22h   10.244.140.209   test-k8s1   <none>           <none>
calico-apiserver   calico-apiserver-85ffbc4979-nrtg6          1/1     Running            2 (31m ago)      2d22h   10.244.140.208   test-k8s1   <none>           <none>
calico-system      calico-kube-controllers-7c8bc859c7-wthcv   1/1     Running            2 (31m ago)      2d22h   10.244.140.207   test-k8s1   <none>           <none>
calico-system      calico-node-5xnfc                          0/1     CrashLoopBackOff   7 (4m7s ago)     27m     10.10.3.92       test-k8s2   <none>           <none>
calico-system      calico-node-n5ppw                          0/1     CrashLoopBackOff   6 (63s ago)      26m     10.10.3.94       test-k8s4   <none>           <none>
calico-system      calico-node-p9sgx                          0/1     Running            5 (112s ago)     26m     10.10.3.95       test-k8s5   <none>           <none>
calico-system      calico-node-r4gj9                          1/1     Running            6 (4m40s ago)    26m     10.10.3.93       test-k8s3   <none>           <none>
calico-system      calico-node-rvr5g                          1/1     Running            2 (31m ago)      2d22h   10.10.3.91       test-k8s1   <none>           <none>
calico-system      calico-typha-6577c4cfd4-mlr4c              1/1     Running            4 (30m ago)      2d22h   10.10.3.91       test-k8s1   <none>           <none>
calico-system      calico-typha-6577c4cfd4-ps4r9              1/1     Running            6 (5m4s ago)     25m     10.10.3.95       test-k8s5   <none>           <none>
calico-system      calico-typha-6577c4cfd4-zxr4d              1/1     Running            6 (3m41s ago)    26m     10.10.3.93       test-k8s3   <none>           <none>
calico-system      csi-node-driver-swmp9                      2/2     Running            10 (11m ago)     26m     10.244.120.70    test-k8s5   <none>           <none>
calico-system      csi-node-driver-tgqwl                      2/2     Running            10 (20s ago)     26m     10.244.117.70    test-k8s4   <none>           <none>
calico-system      csi-node-driver-tt54s                      2/2     Running            8 (11m ago)      27m     10.244.194.69    test-k8s2   <none>           <none>
calico-system      csi-node-driver-vjgr5                      2/2     Running            10 (7m30s ago)   26m     10.244.187.198   test-k8s3   <none>           <none>
calico-system      csi-node-driver-zpmf7                      2/2     Running            4 (31m ago)      2d22h   10.244.140.210   test-k8s1   <none>           <none>
kube-system        coredns-5dd5756b68-h62qn                   1/1     Running            2 (31m ago)      2d22h   10.244.140.206   test-k8s1   <none>           <none>
kube-system        coredns-5dd5756b68-rpm5p                   1/1     Running            2 (31m ago)      2d22h   10.244.140.205   test-k8s1   <none>           <none>
kube-system        etcd-test-k8s1                             1/1     Running            2 (31m ago)      2d22h   10.10.3.91       test-k8s1   <none>           <none>
kube-system        kube-apiserver-test-k8s1                   1/1     Running            2 (31m ago)      2d22h   10.10.3.91       test-k8s1   <none>           <none>
kube-system        kube-controller-manager-test-k8s1          1/1     Running            2 (31m ago)      2d22h   10.10.3.91       test-k8s1   <none>           <none>
kube-system        kube-proxy-pwxwv                           1/1     Running            2 (11m ago)      26m     10.10.3.93       test-k8s3   <none>           <none>
kube-system        kube-proxy-wdcr5                           1/1     Running            6 (9m21s ago)    26m     10.10.3.94       test-k8s4   <none>           <none>
kube-system        kube-proxy-xnqv2                           1/1     Running            4 (5m45s ago)    26m     10.10.3.95       test-k8s5   <none>           <none>
kube-system        kube-proxy-xwfbn                           1/1     Running            5 (3m6s ago)     27m     10.10.3.92       test-k8s2   <none>           <none>
kube-system        kube-proxy-zqczf                           1/1     Running            2 (31m ago)      2d22h   10.10.3.91       test-k8s1   <none>           <none>
kube-system        kube-scheduler-test-k8s1                   1/1     Running            2 (31m ago)      2d22h   10.10.3.91       test-k8s1   <none>           <none>
tigera-operator    tigera-operator-94d7f7696-rrfn2            1/1     Running            4 (30m ago)      2d22h   10.10.3.91       test-k8s1   <none>           <none>

```

```bash
root@test-k8s1:~/hw3.3# kubectl get svc -A  -o wide
NAMESPACE          NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE     SELECTOR
app                back-svc                          ClusterIP   10.245.232.185   <none>        80/TCP,443/TCP           2m8s    app=backend
app                cache-svc                         ClusterIP   10.245.4.5       <none>        80/TCP,443/TCP           2m8s    app=cache
app                front-svc                         ClusterIP   10.245.49.160    <none>        80/TCP,443/TCP           2m8s    app=frontend
calico-apiserver   calico-api                        ClusterIP   10.245.195.66    <none>        443/TCP                  2d22h   apiserver=true
calico-system      calico-kube-controllers-metrics   ClusterIP   None             <none>        9094/TCP                 2d22h   k8s-app=calico-kube-controllers
calico-system      calico-typha                      ClusterIP   10.245.97.128    <none>        5473/TCP                 2d22h   k8s-app=calico-typha
default            kubernetes                        ClusterIP   10.245.0.1       <none>        443/TCP                  2d22h   <none>
kube-system        kube-dns                          ClusterIP   10.245.0.10      <none>        53/UDP,53/TCP,9153/TCP   2d22h   k8s-app=kube-dns

```

```bash
root@test-k8s1:~/hw3.3# kubectl get networkpolicies.networking.k8s.io -A
NAMESPACE          NAME               POD-SELECTOR     AGE
app                app-deny-ingress   <none>           2m34s
app                backend            app=backend      2m34s
app                cache              app=cache        2m34s
calico-apiserver   allow-apiserver    apiserver=true   2d22h

```

```bash
root@test-k8s1:~/hw3.3# kubectl exec --namespace app frontend-856666b494-ntm42 -- curl --max-time 7  -s front-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app frontend-856666b494-ntm42 -- curl --max-time 7  -sk https://front-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app frontend-856666b494-ntm42 -- curl --max-time 7  -s back-svc
WBITT Network MultiTool (with NGINX) - backend-ffdd87cf6-bpfzt - 10.244.187.202 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
root@test-k8s1:~/hw3.3# kubectl exec --namespace app frontend-856666b494-ntm42 -- curl --max-time 7  -sk https://back-svc
WBITT Network MultiTool (with NGINX) - backend-ffdd87cf6-bpfzt - 10.244.187.202 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
root@test-k8s1:~/hw3.3# kubectl exec --namespace app frontend-856666b494-ntm42 -- curl --max-time 7  -s cache-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app frontend-856666b494-ntm42 -- curl --max-time 7  -sk https://cache-svc
command terminated with exit code 28

```

```bash
root@test-k8s1:~/hw3.3# kubectl exec --namespace app backend-ffdd87cf6-bpfzt  -- curl  --max-time 7 -s front-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app backend-ffdd87cf6-bpfzt  -- curl --max-time 7 -sk https://front-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app backend-ffdd87cf6-bpfzt  -- curl  --max-time 7 -s back-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app backend-ffdd87cf6-bpfzt  -- curl --max-time 7 -sk https://back-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app backend-ffdd87cf6-bpfzt  -- curl --max-time 7  -s cache-svc
WBITT Network MultiTool (with NGINX) - cache-79b74c6c94-slnbs - 10.244.187.203 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
root@test-k8s1:~/hw3.3# kubectl exec --namespace app backend-ffdd87cf6-bpfzt  -- curl --max-time 7 -sk https://cache-svc
WBITT Network MultiTool (with NGINX) - cache-79b74c6c94-slnbs - 10.244.187.203 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)

```

```bash
root@test-k8s1:~/hw3.3# kubectl exec --namespace app cache-79b74c6c94-6vlws -- curl --max-time 7  -s front-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app cache-79b74c6c94-6vlws -- curl --max-time 7  -sk https://front-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app cache-79b74c6c94-6vlws -- curl --max-time 7  -s back-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app cache-79b74c6c94-6vlws -- curl --max-time 7  -sk https://back-svc
command terminated with exit code 28
root@test-k8s1:~/hw3.3# kubectl exec --namespace app cache-79b74c6c94-6vlws -- curl --max-time 7  -s cache-svc
command terminated with exit code 137
root@test-k8s1:~/hw3.3# kubectl exec --namespace app cache-79b74c6c94-6vlws -- curl --max-time 7  -sk https://cache-svc
command terminated with exit code 28

```

<details><summary>networkpolicy.yml</summary>

```bash
root@test-k8s1:~/hw3.3# cat networkpolicy.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-deny-ingress
  namespace: app
spec:
  podSelector: {}
  policyTypes:
    - Ingress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend
  namespace: app
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Ingress
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: frontend
      ports:
        - protocol: TCP
          port: 80
        - protocol: TCP
          port: 443
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: cache
  namespace: app
spec:
  podSelector:
    matchLabels:
      app: cache
  policyTypes:
    - Ingress
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: backend
      ports:
        - protocol: TCP
          port: 80
        - protocol: TCP
          port: 443


```

</details>

<details><summary>frontend.yaml</summary>

```bash
root@test-k8s1:~/hw3.3# cat frontend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: app
  labels:
    app: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - name: multitool
        image: wbitt/network-multitool
        ports:
        - containerPort: 80
          name: frontend-http
        - containerPort: 443
          name: frontend-https
---
apiVersion: v1
kind: Service
metadata:
  name: front-svc
  namespace: app
spec:
  selector:
    app: frontend
  ports:
  - name: front-svc-http
    protocol: TCP
    port: 80
    targetPort: frontend-http
  - name: front-svc-https
    protocol: TCP
    port: 443
    targetPort: frontend-https

```

</details>

<details><summary>backend.yaml</summary>

```bash
root@test-k8s1:~/hw3.3# cat backend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: app
  labels:
    app: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: multitool
        image: wbitt/network-multitool
        ports:
        - containerPort: 80
          name: backend-http
        - containerPort: 443
          name: backend-https
---
apiVersion: v1
kind: Service
metadata:
  name: back-svc
  namespace: app
spec:
  selector:
    app: backend
  ports:
  - name: back-svc-http
    protocol: TCP
    port: 80
    targetPort: backend-http
  - name: back-svc-https
    protocol: TCP
    port: 443
    targetPort: backend-https

```

</details>

<details><summary>cache.yaml</summary>

```bash
root@test-k8s1:~/hw3.3# cat cache.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cache
  namespace: app
  labels:
    app: cache
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cache
  template:
    metadata:
      labels:
        app: cache
    spec:
      containers:
      - name: multitool
        image: wbitt/network-multitool
        ports:
        - containerPort: 80
          name: cache-http
        - containerPort: 443
          name: cache-https
---
apiVersion: v1
kind: Service
metadata:
  name: cache-svc
  namespace: app
spec:
  selector:
    app: cache
  ports:
  - name: cache-svc-http
    protocol: TCP
    port: 80
    targetPort: cache-http
  - name: cache-svc-https
    protocol: TCP
    port: 443
    targetPort: cache-https

```

</details>