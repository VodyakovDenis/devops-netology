# Домашнее задание к занятию «Обновление приложений»

### Цель задания

Выбрать и настроить стратегию обновления приложения.

### Чеклист готовности к домашнему заданию

1. Кластер K8s.

```bash
root@test-k8s1:~/hw3.4# kubectl get nodes -A -owide
NAME        STATUS   ROLES           AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
test-k8s1   Ready    control-plane   4d2h   v1.28.4   10.10.3.91    <none>        Ubuntu 22.04.3 LTS   5.15.0-89-generic   containerd://1.7.2
test-k8s2   Ready    worker          28h    v1.28.4   10.10.3.92    <none>        Ubuntu 22.04.3 LTS   5.15.0-89-generic   containerd://1.7.2
test-k8s3   Ready    worker          28h    v1.28.4   10.10.3.93    <none>        Ubuntu 22.04.3 LTS   5.15.0-89-generic   containerd://1.7.2
test-k8s4   Ready    worker          28h    v1.28.4   10.10.3.94    <none>        Ubuntu 22.04.3 LTS   5.15.0-89-generic   containerd://1.7.2
test-k8s5   Ready    worker          28h    v1.28.4   10.10.3.95    <none>        Ubuntu 22.04.3 LTS   5.15.0-89-generic   containerd://1.7.2

```
-----

### Задание 1. Выбрать стратегию обновления приложения и описать ваш выбор

1. Имеется приложение, состоящее из нескольких реплик, которое требуется обновить.
2. Ресурсы, выделенные для приложения, ограничены, и нет возможности их увеличить.
3. Запас по ресурсам в менее загруженный момент времени составляет 20%.
4. Обновление мажорное, новые версии приложения не умеют работать со старыми.
5. Вам нужно объяснить свой выбор стратегии обновления приложения.

### Решение

Маловато вводных данных. как минимум нет пониманием от чего считать 20%, плюс по идее раз приложение мажорное оно уже должно было быть протестировано на отдельной среде.

**Соответвенно полагаю что есть два варианта:**

В менее загруженный момент времени, уменьшить количество реплик текущего приложения до возможного минимума, после чего поднять необходимый минимум нового приложения, перенести нагрузку со старого на новое (к примеру перенастроить балансиры, мы опять же не знаем полной картины того как работает прложение, с чем оно взаимодействует), после чего удалить все реплики старого приложения и добавить необходимое количество реплик нового приложения.

Либо если простой согласован мы можем пойти по пути полного Recreate, опять же учитывая что приложение мажорное оно не совместимо со старым, и оно должно уже было быть протестировано, соответственно в определенный момент времи заменяем все реплики одного приложения на другое.

---

### Задание 2. Обновить приложение

1. Создать deployment приложения с контейнерами nginx и multitool. Версию nginx взять 1.19. Количество реплик — 5.
2. Обновить версию nginx в приложении до версии 1.20, сократив время обновления до минимума. Приложение должно быть доступно.
3. Попытаться обновить nginx до версии 1.28, приложение должно оставаться доступным.
4. Откатиться после неудачного обновления.

### Решение

### 1: Создание deployment приложения с контейнерами nginx и multitool. 

<details><summary>frontend.yaml</summary>

```bash
root@test-k8s1:~/hw3.4# cat frontend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  labels:
    app: nginx
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
          name: nginx-http
---
apiVersion: v1
kind: Service
metadata:
  name: frontend-svc
spec:
  selector:
    app: nginx
  ports:
  - name: frontend80
    protocol: TCP
    port: 80
    targetPort: nginx-http


```

</details>

<details><summary>backend.yaml</summary>

```bash
root@test-k8s1:~/hw3.4# cat backend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  labels:
    app: mtool
spec:
  replicas: 5
  selector:
    matchLabels:
      app: mtool
  template:
    metadata:
      labels:
        app: mtool
    spec:
      containers:
      - name: multitool
        image: wbitt/network-multitool
        ports:
        - containerPort: 80
          name: mtool-http
---
apiVersion: v1
kind: Service
metadata:
  name: backend-svc
spec:
  selector:
    app: mtool
  ports:
  - name: svc-mtool-http
    protocol: TCP
    port: 80
    targetPort: mtool-http

```

</details>

```bash
root@test-k8s1:~/hw3.4# kubectl apply -f frontend.yaml -f backend.yaml
root@test-k8s1:~/hw3.4# kubectl get pods -A -owide | grep defau
default            backend-76f7699d5c-4llpd                   1/1     Running            1 (88s ago)       2m39s   10.244.187.195   test-k8s3   <none>           <none>
default            backend-76f7699d5c-6fbmd                   1/1     Running            0                 2m39s   10.244.194.107   test-k8s2   <none>           <none>
default            backend-76f7699d5c-7j6mt                   1/1     Running            0                 2m39s   10.244.117.72    test-k8s4   <none>           <none>
default            backend-76f7699d5c-frggz                   1/1     Running            0                 2m39s   10.244.120.68    test-k8s5   <none>           <none>
default            backend-76f7699d5c-twf74                   1/1     Running            1 (10s ago)       2m39s   10.244.117.74    test-k8s4   <none>           <none>
default            frontend-8f7b76b94-5msfb                   1/1     Running            1 (95s ago)       2m39s   10.244.194.108   test-k8s2   <none>           <none>
default            frontend-8f7b76b94-d99tt                   1/1     Running            0                 2m39s   10.244.117.71    test-k8s4   <none>           <none>
default            frontend-8f7b76b94-ffrrf                   1/1     Running            0                 2m39s   10.244.120.67    test-k8s5   <none>           <none>
default            frontend-8f7b76b94-jlx8x                   1/1     Running            0                 2m39s   10.244.187.193   test-k8s3   <none>           <none>
default            frontend-8f7b76b94-vjsx5                   1/1     Running            0                 2m39s   10.244.194.105   test-k8s2   <none>           <none>

```

### 2: Обновление версии nginx в приложении до версии 1.20

```bash
root@test-k8s1:~/hw3.4# kubectl set image deployment/frontend nginx=nginx:1.20 --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/frontend image updated
root@test-k8s1:~/hw3.4# kubectl get pods -A -owide | grep defau
default            backend-76f7699d5c-4llpd                   1/1     Running             1 (5m ago)        6m11s   10.244.187.195   test-k8s3   <none>           <none>
default            backend-76f7699d5c-6fbmd                   1/1     Running             0                 6m11s   10.244.194.107   test-k8s2   <none>           <none>
default            backend-76f7699d5c-7j6mt                   1/1     Running             0                 6m11s   10.244.117.72    test-k8s4   <none>           <none>
default            backend-76f7699d5c-frggz                   1/1     Running             2 (2m15s ago)     6m11s   10.244.120.71    test-k8s5   <none>           <none>
default            backend-76f7699d5c-twf74                   1/1     Running             1 (3m42s ago)     6m11s   10.244.117.74    test-k8s4   <none>           <none>
default            frontend-6c8b776886-d6c9b                  0/1     ContainerCreating   0                 3s      <none>           test-k8s2   <none>           <none>
default            frontend-6c8b776886-fn5f6                  0/1     ContainerCreating   0                 3s      <none>           test-k8s3   <none>           <none>
default            frontend-6c8b776886-tzhd9                  0/1     ContainerCreating   0                 3s      <none>           test-k8s4   <none>           <none>
default            frontend-8f7b76b94-5msfb                   1/1     Running             1 (5m7s ago)      6m11s   10.244.194.108   test-k8s2   <none>           <none>
default            frontend-8f7b76b94-d99tt                   1/1     Running             1 (88s ago)       6m11s   10.244.117.75    test-k8s4   <none>           <none>
default            frontend-8f7b76b94-ffrrf                   1/1     Running             1 (52s ago)       6m11s   10.244.120.72    test-k8s5   <none>           <none>
default            frontend-8f7b76b94-jlx8x                   1/1     Running             0                 6m11s   10.244.187.193   test-k8s3   <none>           <none>
root@test-k8s1:~/hw3.4# kubectl get pods -A -owide | grep defau
default            backend-76f7699d5c-4llpd                   1/1     Running            1 (5m19s ago)     6m30s   10.244.187.195   test-k8s3   <none>           <none>
default            backend-76f7699d5c-6fbmd                   1/1     Running            0                 6m30s   10.244.194.107   test-k8s2   <none>           <none>
default            backend-76f7699d5c-7j6mt                   1/1     Running            0                 6m30s   10.244.117.72    test-k8s4   <none>           <none>
default            backend-76f7699d5c-frggz                   1/1     Running            2 (2m34s ago)     6m30s   10.244.120.71    test-k8s5   <none>           <none>
default            backend-76f7699d5c-twf74                   1/1     Running            1 (4m1s ago)      6m30s   10.244.117.74    test-k8s4   <none>           <none>
default            frontend-6c8b776886-2npg8                  1/1     Running            0                 12s     10.244.120.73    test-k8s5   <none>           <none>
default            frontend-6c8b776886-d6c9b                  1/1     Running            0                 22s     10.244.194.111   test-k8s2   <none>           <none>
default            frontend-6c8b776886-fn5f6                  1/1     Running            0                 22s     10.244.187.196   test-k8s3   <none>           <none>
default            frontend-6c8b776886-l2zhc                  1/1     Running            0                 11s     10.244.194.113   test-k8s2   <none>           <none>
default            frontend-6c8b776886-tzhd9                  1/1     Running            0                 22s     10.244.117.76    test-k8s4   <none>           <none>

```

**Немного лога:**

```bash
root@test-k8s1:~/hw3.4# kubectl describe pods

***

Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  6m54s  default-scheduler  Successfully assigned default/frontend-6c8b776886-tzhd9 to test-k8s4
  Normal  Pulling    6m53s  kubelet            Pulling image "nginx:1.20"
  Normal  Pulled     6m44s  kubelet            Successfully pulled image "nginx:1.20" in 9.093s (9.093s including waiting)
  Normal  Created    6m44s  kubelet            Created container nginx
  Normal  Started    6m44s  kubelet            Started container nginx

```

### 3: Попытка обновить nginx до версии 1.28

```bash
root@test-k8s1:~/hw3.4# kubectl set image deployment/frontend nginx=nginx:1.28 --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/frontend image updated
root@test-k8s1:~/hw3.4# kubectl get pods -A -owide | grep defau
default            backend-76f7699d5c-4llpd                   1/1     Running            3 (6m31s ago)     24m    10.244.187.206   test-k8s3   <none>           <none>
default            backend-76f7699d5c-6fbmd                   1/1     Running            1 (2m53s ago)     24m    10.244.194.65    test-k8s2   <none>           <none>
default            backend-76f7699d5c-7j6mt                   1/1     Running            1 (4m4s ago)      24m    10.244.117.82    test-k8s4   <none>           <none>
default            backend-76f7699d5c-frggz                   1/1     Running            3 (2m44s ago)     24m    10.244.120.79    test-k8s5   <none>           <none>
default            backend-76f7699d5c-twf74                   1/1     Running            3 (15m ago)       24m    10.244.117.79    test-k8s4   <none>           <none>
default            frontend-5cfcc876d6-8kq9w                  0/1     ErrImagePull       0                 7s     10.244.187.208   test-k8s3   <none>           <none>
default            frontend-5cfcc876d6-gjdnx                  0/1     ErrImagePull       0                 7s     10.244.120.80    test-k8s5   <none>           <none>
default            frontend-5cfcc876d6-vc564                  0/1     ErrImagePull       0                 7s     10.244.194.68    test-k8s2   <none>           <none>
default            frontend-6c8b776886-2npg8                  1/1     Running            1 (4m7s ago)      18m    10.244.120.77    test-k8s5   <none>           <none>
default            frontend-6c8b776886-fn5f6                  1/1     Running            1 (9m28s ago)     18m    10.244.187.204   test-k8s3   <none>           <none>
default            frontend-6c8b776886-l2zhc                  1/1     Running            4 (5m45s ago)     18m    10.244.194.124   test-k8s2   <none>           <none>
default            frontend-6c8b776886-tzhd9                  1/1     Running            1 (114s ago)      18m    10.244.117.83    test-k8s4   <none>           <none>

```

```bash
root@test-k8s1:~/hw3.4# kubectl describe pods frontend-5cfcc876d6-8kq9w | grep nginx
Labels:           app=nginx
  nginx:
    Image:          nginx:1.28
  Normal   Pulling    14s (x3 over 58s)  kubelet            Pulling image "nginx:1.28"
  Warning  Failed     12s (x3 over 57s)  kubelet            Failed to pull image "nginx:1.28": rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/nginx:1.28": failed to resolve reference "docker.io/library/nginx:1.28": docker.io/library/nginx:1.28: not found
  Normal   BackOff    1s (x3 over 57s)   kubelet            Back-off pulling image "nginx:1.28"


```

Обновление до версии 1.28 вызывает проблемы, приложение должно оставаться доступным, так как у нас есть несколько реплик.

### 4: Откат к предыдущей версии после неудачного обновления.

```bash
root@test-k8s1:~/hw3.4# kubectl rollout undo deployment/frontend
deployment.apps/frontend rolled back
root@test-k8s1:~/hw3.4# kubectl get pods -A -owide | grep defau
default            backend-76f7699d5c-4llpd                   1/1     Running            3 (2m3s ago)      20m    10.244.187.206   test-k8s3   <none>           <none>
default            backend-76f7699d5c-6fbmd                   1/1     Running            0                 20m    10.244.194.107   test-k8s2   <none>           <none>
default            backend-76f7699d5c-7j6mt                   1/1     Running            0                 20m    10.244.117.72    test-k8s4   <none>           <none>
default            backend-76f7699d5c-frggz                   1/1     Running            2 (16m ago)       20m    10.244.120.71    test-k8s5   <none>           <none>
default            backend-76f7699d5c-twf74                   1/1     Running            3 (10m ago)       20m    10.244.117.79    test-k8s4   <none>           <none>
default            frontend-6c8b776886-2npg8                  1/1     Running            0                 13m    10.244.120.73    test-k8s5   <none>           <none>
default            frontend-6c8b776886-fn5f6                  1/1     Running            1 (5m ago)        14m    10.244.187.204   test-k8s3   <none>           <none>
default            frontend-6c8b776886-l2zhc                  1/1     Running            4 (77s ago)       13m    10.244.194.124   test-k8s2   <none>           <none>
default            frontend-6c8b776886-tzhd9                  1/1     Running            0                 14m    10.244.117.76    test-k8s4   <none>           <none>
default            frontend-6c8b776886-xldkg                  1/1     Running            0                 2s     10.244.194.125   test-k8s2   <none>           <none>

```

```bash
root@test-k8s1:~/hw3.4# kubectl describe pods frontend-6c8b776886-fn5f6 | grep nginx
Labels:           app=nginx
  nginx:
    Image:          nginx:1.20
    Image ID:       docker.io/library/nginx@sha256:38f8c1d9613f3f42e7969c3b1dd5c3277e635d4576713e6453c6193e66270a6d
  Normal  Pulling         17m                  kubelet            Pulling image "nginx:1.20"
  Normal  Pulled          17m                  kubelet            Successfully pulled image "nginx:1.20" in 8.953s (8.953s including waiting)
  Normal  Killing         8m34s                kubelet            Stopping container nginx
  Normal  Created         8m32s (x2 over 17m)  kubelet            Created container nginx
  Normal  Started         8m32s (x2 over 17m)  kubelet            Started container nginx
  Normal  Pulled          8m32s                kubelet            Container image "nginx:1.20" already present on machine
anged, it will be killed and re-created.

```

Обновление откатилось.